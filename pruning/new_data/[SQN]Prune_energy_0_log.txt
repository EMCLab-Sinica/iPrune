
==> Dump Arguments:
pruning method: energy
visible gpus: 2
sensitivity analysis: OFF
overall pruning ratio: 0.2
stage: 0

0
0

==> Setting params:
batch_size : 128
test_batch_size : 128
epochs : 150
lr_epochs : 50
lr : 0.0001
momentum : 0.9
weight_decay : 0.0001
no_cuda : False
seed : 1
log_interval : 100
arch : SqueezeNet
pretrained : saved_models/SqueezeNet.origin.pth.tar
evaluate : False
retrain : False
prune : energy
prune_shape : vector
prune_target : None
stage : 0
debug : -1
candidates_pruning_ratios : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
gpus : [0]
visible_gpus : 2
learning_rate_list : [0.001, 0.0005]
sa : True
sen_ana : False
overall_pruning_ratio : 0.2
cuda : False
====================

Files already downloaded and verified
SqueezeNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))
  (relu_conv1): ReLU(inplace=True)
  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fire1): Fire(
    (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
    (relu_squeeze): ReLU(inplace=True)
    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
    (relu_expand1x1): ReLU(inplace=True)
    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_expand3x3): ReLU(inplace=True)
  )
  (fire2): Fire(
    (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
    (relu_squeeze): ReLU(inplace=True)
    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
    (relu_expand1x1): ReLU(inplace=True)
    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_expand3x3): ReLU(inplace=True)
  )
  (fire3): Fire(
    (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
    (relu_squeeze): ReLU(inplace=True)
    (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
    (relu_expand1x1): ReLU(inplace=True)
    (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_expand3x3): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.5, inplace=False)
  (conv2): Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))
  (relu_conv2): ReLU(inplace=True)
  (pool2): AdaptiveAvgPool2d(output_size=(1, 1))
  (softmax): Softmax(dim=1)
)
==> Start pruning ...
[0.013573581733920866, 0.0014908705445676484, 0.0017813307845676492, 0.005653094501108835, 0.002884921009135297, 0.00173776670754991, 0.005653094501108835, 0.005769842018270596, 0.00635076249827059, 0.016493349129636938, 0.003545638711419121]
===> Starting Simulated Annealing...
[Iter 0] cur_temp: 100.00 | stop_temp: 20.00 | Loss: -inf | Best perf: -inf[Iter 1] cur_temp: 90.00 | stop_temp: 20.00 | Loss: -2.2351 | Best perf: -2.2351[Iter 2] cur_temp: 81.00 | stop_temp: 20.00 | Loss: -2.2665 | Best perf: -2.2351[Iter 3] cur_temp: 72.90 | stop_temp: 20.00 | Loss: -2.2589 | Best perf: -2.2351[Iter 4] cur_temp: 65.61 | stop_temp: 20.00 | Loss: -2.2180 | Best perf: -2.2180[Iter 5] cur_temp: 59.05 | stop_temp: 20.00 | Loss: -2.2288 | Best perf: -2.2180[Iter 6] cur_temp: 53.14 | stop_temp: 20.00 | Loss: -2.2483 | Best perf: -2.2180[Iter 7] cur_temp: 47.83 | stop_temp: 20.00 | Loss: -2.2464 | Best perf: -2.2180[Iter 8] cur_temp: 43.05 | stop_temp: 20.00 | Loss: -2.2473 | Best perf: -2.2180[Iter 9] cur_temp: 38.74 | stop_temp: 20.00 | Loss: -2.3102 | Best perf: -2.2180[Iter 10] cur_temp: 34.87 | stop_temp: 20.00 | Loss: -2.2456 | Best perf: -2.2180[Iter 11] cur_temp: 31.38 | stop_temp: 20.00 | Loss: -2.2404 | Best perf: -2.2180[Iter 12] cur_temp: 28.24 | stop_temp: 20.00 | Loss: -2.2339 | Best perf: -2.2180[Iter 13] cur_temp: 25.42 | stop_temp: 20.00 | Loss: -2.3316 | Best perf: -2.2180[Iter 14] cur_temp: 22.88 | stop_temp: 20.00 | Loss: -2.2282 | Best perf: -2.2180[Iter 15] cur_temp: 20.59 | stop_temp: 20.00 | Loss: -2.2508 | Best perf: -2.2180
------------------------------------------------------------------
- Intermittent-aware weight pruning info:
- Layer 0:        216 /       1728 (12.5%) weights are pruned
- Layer 1:          0 /       1024 ( 0.0%) weights are pruned
- Layer 2:          0 /       1024 ( 0.0%) weights are pruned
- Layer 3:        352 /       9216 ( 3.8%) weights are pruned
- Layer 4:          0 /       2048 ( 0.0%) weights are pruned
- Layer 5:          0 /       1024 ( 0.0%) weights are pruned
- Layer 6:        512 /       9216 ( 5.6%) weights are pruned
- Layer 7:        352 /       4096 ( 8.6%) weights are pruned
- Layer 8:        384 /       4096 ( 9.4%) weights are pruned
- Layer 9:      12608 /      36864 (34.2%) weights are pruned
- Layer 10:         32 /       2560 ( 1.2%) weights are pruned
- Total  :      14456 /      72896 (19.8%) weights are pruned
------------------------------------------------------------------

[Epoch: 1| Loss: 1.9030| Accuracy: 58.40| Best Accuracy: 58.40]
[Epoch: 2| Loss: 1.9103| Accuracy: 57.33| Best Accuracy: 58.40]
[Epoch: 3| Loss: 1.8921| Accuracy: 59.34| Best Accuracy: 59.34]
[Epoch: 4| Loss: 1.8895| Accuracy: 59.34| Best Accuracy: 59.34]
[Epoch: 5| Loss: 1.8868| Accuracy: 59.39| Best Accuracy: 59.39]
[Epoch: 6| Loss: 1.8872| Accuracy: 59.61| Best Accuracy: 59.61]
[Epoch: 7| Loss: 1.8800| Accuracy: 60.01| Best Accuracy: 60.01]
[Epoch: 8| Loss: 1.8825| Accuracy: 60.03| Best Accuracy: 60.03]
[Epoch: 9| Loss: 1.8883| Accuracy: 59.85| Best Accuracy: 60.03]
[Epoch: 10| Loss: 1.8828| Accuracy: 59.76| Best Accuracy: 60.03]
[Epoch: 11| Loss: 1.8887| Accuracy: 59.57| Best Accuracy: 60.03]
[Epoch: 12| Loss: 1.8937| Accuracy: 58.68| Best Accuracy: 60.03]
[Epoch: 13| Loss: 1.8832| Accuracy: 59.84| Best Accuracy: 60.03]
[Epoch: 14| Loss: 1.8799| Accuracy: 60.39| Best Accuracy: 60.39]
[Epoch: 15| Loss: 1.8828| Accuracy: 59.90| Best Accuracy: 60.39]
[Epoch: 16| Loss: 1.8819| Accuracy: 60.29| Best Accuracy: 60.39]
[Epoch: 17| Loss: 1.8832| Accuracy: 60.20| Best Accuracy: 60.39]
[Epoch: 18| Loss: 1.8755| Accuracy: 60.94| Best Accuracy: 60.94]
[Epoch: 19| Loss: 1.8825| Accuracy: 59.94| Best Accuracy: 60.94]
[Epoch: 20| Loss: 1.8768| Accuracy: 60.23| Best Accuracy: 60.94]
[Epoch: 21| Loss: 1.8732| Accuracy: 61.06| Best Accuracy: 61.06]
[Epoch: 22| Loss: 1.8776| Accuracy: 60.58| Best Accuracy: 61.06]
[Epoch: 23| Loss: 1.8763| Accuracy: 60.69| Best Accuracy: 61.06]
[Epoch: 24| Loss: 1.8728| Accuracy: 60.85| Best Accuracy: 61.06]
[Epoch: 25| Loss: 1.8730| Accuracy: 60.86| Best Accuracy: 61.06]
[Epoch: 26| Loss: 1.8736| Accuracy: 60.76| Best Accuracy: 61.06]
[Epoch: 27| Loss: 1.8727| Accuracy: 60.64| Best Accuracy: 61.06]
[Epoch: 28| Loss: 1.8732| Accuracy: 60.59| Best Accuracy: 61.06]
[Epoch: 29| Loss: 1.8766| Accuracy: 60.79| Best Accuracy: 61.06]
[Epoch: 30| Loss: 1.8750| Accuracy: 60.94| Best Accuracy: 61.06]
[Epoch: 31| Loss: 1.8676| Accuracy: 61.40| Best Accuracy: 61.40]
[Epoch: 32| Loss: 1.8724| Accuracy: 60.85| Best Accuracy: 61.40]
[Epoch: 33| Loss: 1.8712| Accuracy: 61.23| Best Accuracy: 61.40]
[Epoch: 34| Loss: 1.8720| Accuracy: 60.88| Best Accuracy: 61.40]
[Epoch: 35| Loss: 1.8663| Accuracy: 61.57| Best Accuracy: 61.57]
[Epoch: 36| Loss: 1.8673| Accuracy: 61.29| Best Accuracy: 61.57]
[Epoch: 37| Loss: 1.8678| Accuracy: 61.53| Best Accuracy: 61.57]
[Epoch: 38| Loss: 1.8664| Accuracy: 61.79| Best Accuracy: 61.79]
[Epoch: 39| Loss: 1.8669| Accuracy: 61.90| Best Accuracy: 61.90]
[Epoch: 40| Loss: 1.8706| Accuracy: 61.19| Best Accuracy: 61.90]
[Epoch: 41| Loss: 1.8659| Accuracy: 61.45| Best Accuracy: 61.90]
[Epoch: 42| Loss: 1.8682| Accuracy: 61.51| Best Accuracy: 61.90]
[Epoch: 43| Loss: 1.8663| Accuracy: 61.56| Best Accuracy: 61.90]
[Epoch: 44| Loss: 1.8618| Accuracy: 61.99| Best Accuracy: 61.99]
[Epoch: 45| Loss: 1.8647| Accuracy: 62.07| Best Accuracy: 62.07]
[Epoch: 46| Loss: 1.8621| Accuracy: 61.92| Best Accuracy: 62.07]
[Epoch: 47| Loss: 1.8612| Accuracy: 61.97| Best Accuracy: 62.07]
[Epoch: 48| Loss: 1.8627| Accuracy: 62.22| Best Accuracy: 62.22]
[Epoch: 49| Loss: 1.8672| Accuracy: 61.53| Best Accuracy: 62.22]
adjusting learning rate to 0.001 ...
[Epoch: 50| Loss: 1.9217| Accuracy: 55.97| Best Accuracy: 62.22]
[Epoch: 51| Loss: 1.9041| Accuracy: 57.60| Best Accuracy: 62.22]
[Epoch: 52| Loss: 1.9175| Accuracy: 55.93| Best Accuracy: 62.22]
[Epoch: 53| Loss: 1.8992| Accuracy: 57.93| Best Accuracy: 62.22]
[Epoch: 54| Loss: 1.9232| Accuracy: 55.95| Best Accuracy: 62.22]
[Epoch: 55| Loss: 1.9034| Accuracy: 57.45| Best Accuracy: 62.22]
[Epoch: 56| Loss: 1.8886| Accuracy: 59.45| Best Accuracy: 62.22]
[Epoch: 57| Loss: 1.9152| Accuracy: 56.53| Best Accuracy: 62.22]
[Epoch: 58| Loss: 1.8799| Accuracy: 60.15| Best Accuracy: 62.22]
[Epoch: 59| Loss: 1.8765| Accuracy: 60.50| Best Accuracy: 62.22]
[Epoch: 60| Loss: 1.8619| Accuracy: 61.64| Best Accuracy: 62.22]
[Epoch: 61| Loss: 1.8949| Accuracy: 58.82| Best Accuracy: 62.22]
[Epoch: 62| Loss: 1.8649| Accuracy: 61.68| Best Accuracy: 62.22]
[Epoch: 63| Loss: 1.8480| Accuracy: 63.26| Best Accuracy: 63.26]
[Epoch: 64| Loss: 1.8772| Accuracy: 59.87| Best Accuracy: 63.26]
[Epoch: 65| Loss: 1.8578| Accuracy: 62.29| Best Accuracy: 63.26]
[Epoch: 66| Loss: 1.8500| Accuracy: 63.23| Best Accuracy: 63.26]
[Epoch: 67| Loss: 1.8339| Accuracy: 64.44| Best Accuracy: 64.44]
[Epoch: 68| Loss: 1.8442| Accuracy: 63.65| Best Accuracy: 64.44]
[Epoch: 69| Loss: 1.8516| Accuracy: 63.03| Best Accuracy: 64.44]
[Epoch: 70| Loss: 1.8432| Accuracy: 63.57| Best Accuracy: 64.44]
[Epoch: 71| Loss: 1.8303| Accuracy: 64.77| Best Accuracy: 64.77]
[Epoch: 72| Loss: 1.8479| Accuracy: 63.28| Best Accuracy: 64.77]
[Epoch: 73| Loss: 1.8558| Accuracy: 62.35| Best Accuracy: 64.77]
[Epoch: 74| Loss: 1.8343| Accuracy: 64.51| Best Accuracy: 64.77]
[Epoch: 75| Loss: 1.8174| Accuracy: 66.36| Best Accuracy: 66.36]
[Epoch: 76| Loss: 1.8405| Accuracy: 64.05| Best Accuracy: 66.36]
[Epoch: 77| Loss: 1.8303| Accuracy: 65.03| Best Accuracy: 66.36]
[Epoch: 78| Loss: 1.8169| Accuracy: 66.18| Best Accuracy: 66.36]
[Epoch: 79| Loss: 1.8212| Accuracy: 65.81| Best Accuracy: 66.36]
[Epoch: 80| Loss: 1.8103| Accuracy: 67.33| Best Accuracy: 67.33]
[Epoch: 81| Loss: 1.8026| Accuracy: 67.68| Best Accuracy: 67.68]
[Epoch: 82| Loss: 1.8309| Accuracy: 65.01| Best Accuracy: 67.68]
[Epoch: 83| Loss: 1.7962| Accuracy: 68.56| Best Accuracy: 68.56]
[Epoch: 84| Loss: 1.7937| Accuracy: 69.33| Best Accuracy: 69.33]
[Epoch: 85| Loss: 1.7906| Accuracy: 68.98| Best Accuracy: 69.33]
[Epoch: 86| Loss: 1.8205| Accuracy: 65.96| Best Accuracy: 69.33]
[Epoch: 87| Loss: 1.7888| Accuracy: 69.43| Best Accuracy: 69.43]
[Epoch: 88| Loss: 1.8078| Accuracy: 67.04| Best Accuracy: 69.43]
[Epoch: 89| Loss: 1.7829| Accuracy: 69.76| Best Accuracy: 69.76]
[Epoch: 90| Loss: 1.7800| Accuracy: 69.95| Best Accuracy: 69.95]
[Epoch: 91| Loss: 1.7880| Accuracy: 69.05| Best Accuracy: 69.95]
[Epoch: 92| Loss: 1.7991| Accuracy: 68.08| Best Accuracy: 69.95]
[Epoch: 93| Loss: 1.7917| Accuracy: 68.77| Best Accuracy: 69.95]
[Epoch: 94| Loss: 1.7873| Accuracy: 69.33| Best Accuracy: 69.95]
[Epoch: 95| Loss: 1.7828| Accuracy: 69.77| Best Accuracy: 69.95]
[Epoch: 96| Loss: 1.8022| Accuracy: 67.82| Best Accuracy: 69.95]
[Epoch: 97| Loss: 1.7772| Accuracy: 70.52| Best Accuracy: 70.52]
[Epoch: 98| Loss: 1.7856| Accuracy: 69.19| Best Accuracy: 70.52]
[Epoch: 99| Loss: 1.7751| Accuracy: 70.53| Best Accuracy: 70.53]
adjusting learning rate to 0.0005 ...
[Epoch: 100| Loss: 1.7613| Accuracy: 71.47| Best Accuracy: 71.47]
[Epoch: 101| Loss: 1.7625| Accuracy: 71.56| Best Accuracy: 71.56]
[Epoch: 102| Loss: 1.7579| Accuracy: 72.31| Best Accuracy: 72.31]
[Epoch: 103| Loss: 1.7658| Accuracy: 71.49| Best Accuracy: 72.31]
[Epoch: 104| Loss: 1.7565| Accuracy: 72.13| Best Accuracy: 72.31]
[Epoch: 105| Loss: 1.7547| Accuracy: 72.36| Best Accuracy: 72.36]
[Epoch: 106| Loss: 1.7639| Accuracy: 71.89| Best Accuracy: 72.36]
[Epoch: 107| Loss: 1.7586| Accuracy: 71.87| Best Accuracy: 72.36]
[Epoch: 108| Loss: 1.7474| Accuracy: 73.21| Best Accuracy: 73.21]
[Epoch: 109| Loss: 1.7541| Accuracy: 72.70| Best Accuracy: 73.21]
[Epoch: 110| Loss: 1.7538| Accuracy: 72.81| Best Accuracy: 73.21]
[Epoch: 111| Loss: 1.7539| Accuracy: 72.63| Best Accuracy: 73.21]
[Epoch: 112| Loss: 1.7475| Accuracy: 73.37| Best Accuracy: 73.37]
[Epoch: 113| Loss: 1.7507| Accuracy: 72.97| Best Accuracy: 73.37]
[Epoch: 114| Loss: 1.7602| Accuracy: 72.08| Best Accuracy: 73.37]
[Epoch: 115| Loss: 1.7525| Accuracy: 72.56| Best Accuracy: 73.37]
[Epoch: 116| Loss: 1.7729| Accuracy: 70.61| Best Accuracy: 73.37]
[Epoch: 117| Loss: 1.7488| Accuracy: 73.31| Best Accuracy: 73.37]
[Epoch: 118| Loss: 1.7457| Accuracy: 73.61| Best Accuracy: 73.61]
[Epoch: 119| Loss: 1.7518| Accuracy: 72.65| Best Accuracy: 73.61]
[Epoch: 120| Loss: 1.7451| Accuracy: 73.94| Best Accuracy: 73.94]
[Epoch: 121| Loss: 1.7421| Accuracy: 73.81| Best Accuracy: 73.94]
[Epoch: 122| Loss: 1.7428| Accuracy: 73.89| Best Accuracy: 73.94]
[Epoch: 123| Loss: 1.7421| Accuracy: 73.93| Best Accuracy: 73.94]
[Epoch: 124| Loss: 1.7416| Accuracy: 74.08| Best Accuracy: 74.08]
[Epoch: 125| Loss: 1.7449| Accuracy: 73.74| Best Accuracy: 74.08]
[Epoch: 126| Loss: 1.7446| Accuracy: 73.88| Best Accuracy: 74.08]
[Epoch: 127| Loss: 1.7507| Accuracy: 73.21| Best Accuracy: 74.08]
[Epoch: 128| Loss: 1.7363| Accuracy: 74.71| Best Accuracy: 74.71]
[Epoch: 129| Loss: 1.7542| Accuracy: 72.99| Best Accuracy: 74.71]
[Epoch: 130| Loss: 1.7410| Accuracy: 73.90| Best Accuracy: 74.71]
[Epoch: 131| Loss: 1.7446| Accuracy: 73.66| Best Accuracy: 74.71]
[Epoch: 132| Loss: 1.7415| Accuracy: 73.90| Best Accuracy: 74.71]
[Epoch: 133| Loss: 1.7371| Accuracy: 74.27| Best Accuracy: 74.71]
[Epoch: 134| Loss: 1.7369| Accuracy: 74.58| Best Accuracy: 74.71]
[Epoch: 135| Loss: 1.7379| Accuracy: 74.67| Best Accuracy: 74.71]
[Epoch: 136| Loss: 1.7399| Accuracy: 74.39| Best Accuracy: 74.71]
[Epoch: 137| Loss: 1.7389| Accuracy: 74.20| Best Accuracy: 74.71]
[Epoch: 138| Loss: 1.7551| Accuracy: 72.23| Best Accuracy: 74.71]
[Epoch: 139| Loss: 1.7375| Accuracy: 74.16| Best Accuracy: 74.71]
[Epoch: 140| Loss: 1.7307| Accuracy: 75.02| Best Accuracy: 75.02]
[Epoch: 141| Loss: 1.7394| Accuracy: 74.10| Best Accuracy: 75.02]
[Epoch: 142| Loss: 1.7419| Accuracy: 73.74| Best Accuracy: 75.02]
[Epoch: 143| Loss: 1.7412| Accuracy: 74.13| Best Accuracy: 75.02]
[Epoch: 144| Loss: 1.7342| Accuracy: 74.60| Best Accuracy: 75.02]
[Epoch: 145| Loss: 1.7491| Accuracy: 73.18| Best Accuracy: 75.02]
[Epoch: 146| Loss: 1.7300| Accuracy: 75.05| Best Accuracy: 75.05]
[Epoch: 147| Loss: 1.7292| Accuracy: 74.95| Best Accuracy: 75.05]
[Epoch: 148| Loss: 1.7359| Accuracy: 74.23| Best Accuracy: 75.05]
[Epoch: 149| Loss: 1.7363| Accuracy: 74.31| Best Accuracy: 75.05]
