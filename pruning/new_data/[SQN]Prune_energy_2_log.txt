
==> Dump Arguments:
pruning method: energy
visible gpus: 2
sensitivity analysis: OFF
overall pruning ratio: 0.2
stage: 2

0
0

==> Setting params:
batch_size : 128
test_batch_size : 128
epochs : 150
lr_epochs : 50
lr : 0.0001
momentum : 0.9
weight_decay : 0.0001
no_cuda : False
seed : 1
log_interval : 100
arch : SqueezeNet
pretrained : saved_models/energy/SqueezeNet/stage_1.pth.tar
evaluate : False
retrain : False
prune : energy
prune_shape : vector
prune_target : None
stage : 2
debug : -1
candidates_pruning_ratios : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
gpus : [0]
visible_gpus : 2
learning_rate_list : [0.001, 0.0005]
sa : True
sen_ana : False
overall_pruning_ratio : 0.2
cuda : False
====================

Files already downloaded and verified
SqueezeNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))
  (relu_conv1): ReLU(inplace=True)
  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fire1): Fire(
    (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
    (relu_squeeze): ReLU(inplace=True)
    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
    (relu_expand1x1): ReLU(inplace=True)
    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_expand3x3): ReLU(inplace=True)
  )
  (fire2): Fire(
    (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
    (relu_squeeze): ReLU(inplace=True)
    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
    (relu_expand1x1): ReLU(inplace=True)
    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_expand3x3): ReLU(inplace=True)
  )
  (fire3): Fire(
    (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
    (relu_squeeze): ReLU(inplace=True)
    (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
    (relu_expand1x1): ReLU(inplace=True)
    (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_expand3x3): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.5, inplace=False)
  (conv2): Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))
  (relu_conv2): ReLU(inplace=True)
  (pool2): AdaptiveAvgPool2d(output_size=(1, 1))
  (softmax): Softmax(dim=1)
)
==> Start pruning ...
[0.011600596300440651, 0.0014908705445676484, 0.0017813307845676492, 0.005262616113558926, 0.002884921009135297, 0.0017813307845676492, 0.005250020036541186, 0.00507281678598677, 0.005392352803880333, 0.009829491151977208, 0.0034585105573836433]
===> Starting Simulated Annealing...
[Iter 0] cur_temp: 100.00 | stop_temp: 20.00 | Loss: -inf | Best perf: -inf[Iter 1] cur_temp: 90.00 | stop_temp: 20.00 | Loss: -2.0843 | Best perf: -2.0843[Iter 2] cur_temp: 81.00 | stop_temp: 20.00 | Loss: -2.1023 | Best perf: -2.0843[Iter 3] cur_temp: 72.90 | stop_temp: 20.00 | Loss: -2.1675 | Best perf: -2.0843[Iter 4] cur_temp: 65.61 | stop_temp: 20.00 | Loss: -2.1142 | Best perf: -2.0843[Iter 5] cur_temp: 59.05 | stop_temp: 20.00 | Loss: -2.1901 | Best perf: -2.0843[Iter 6] cur_temp: 53.14 | stop_temp: 20.00 | Loss: -2.1352 | Best perf: -2.0843[Iter 7] cur_temp: 47.83 | stop_temp: 20.00 | Loss: -2.1817 | Best perf: -2.0843[Iter 8] cur_temp: 43.05 | stop_temp: 20.00 | Loss: -2.3066 | Best perf: -2.0843[Iter 9] cur_temp: 38.74 | stop_temp: 20.00 | Loss: -2.3673 | Best perf: -2.0843[Iter 10] cur_temp: 34.87 | stop_temp: 20.00 | Loss: -2.3747 | Best perf: -2.0843[Iter 11] cur_temp: 31.38 | stop_temp: 20.00 | Loss: -2.3239 | Best perf: -2.0843[Iter 12] cur_temp: 28.24 | stop_temp: 20.00 | Loss: -2.3337 | Best perf: -2.0843[Iter 13] cur_temp: 25.42 | stop_temp: 20.00 | Loss: -2.1497 | Best perf: -2.0843[Iter 14] cur_temp: 22.88 | stop_temp: 20.00 | Loss: -2.2755 | Best perf: -2.0843[Iter 15] cur_temp: 20.59 | stop_temp: 20.00 | Loss: -2.1076 | Best perf: -2.0843
------------------------------------------------------------------
- Intermittent-aware weight pruning info:
- Layer 0:        852 /       1728 (49.3%) weights are pruned
- Layer 1:          0 /       1024 ( 0.0%) weights are pruned
- Layer 2:          0 /       1024 ( 0.0%) weights are pruned
- Layer 3:       2304 /       9216 (25.0%) weights are pruned
- Layer 4:         64 /       2048 ( 3.1%) weights are pruned
- Layer 5:          0 /       1024 ( 0.0%) weights are pruned
- Layer 6:       1568 /       9216 (17.0%) weights are pruned
- Layer 7:        736 /       4096 (18.0%) weights are pruned
- Layer 8:       1376 /       4096 (33.6%) weights are pruned
- Layer 9:      25408 /      36864 (68.9%) weights are pruned
- Layer 10:        160 /       2560 ( 6.2%) weights are pruned
- Total  :      32468 /      72896 (44.5%) weights are pruned
------------------------------------------------------------------

[Epoch: 1| Loss: 1.7737| Accuracy: 71.10| Best Accuracy: 71.10]
[Epoch: 2| Loss: 1.7497| Accuracy: 73.00| Best Accuracy: 73.00]
[Epoch: 3| Loss: 1.7411| Accuracy: 74.34| Best Accuracy: 74.34]
[Epoch: 4| Loss: 1.7369| Accuracy: 74.65| Best Accuracy: 74.65]
[Epoch: 5| Loss: 1.7307| Accuracy: 74.89| Best Accuracy: 74.89]
[Epoch: 6| Loss: 1.7286| Accuracy: 75.07| Best Accuracy: 75.07]
[Epoch: 7| Loss: 1.7256| Accuracy: 75.52| Best Accuracy: 75.52]
[Epoch: 8| Loss: 1.7243| Accuracy: 75.48| Best Accuracy: 75.52]
[Epoch: 9| Loss: 1.7270| Accuracy: 75.80| Best Accuracy: 75.80]
[Epoch: 10| Loss: 1.7232| Accuracy: 75.86| Best Accuracy: 75.86]
[Epoch: 11| Loss: 1.7210| Accuracy: 76.06| Best Accuracy: 76.06]
[Epoch: 12| Loss: 1.7241| Accuracy: 75.99| Best Accuracy: 76.06]
[Epoch: 13| Loss: 1.7191| Accuracy: 75.92| Best Accuracy: 76.06]
[Epoch: 14| Loss: 1.7184| Accuracy: 76.47| Best Accuracy: 76.47]
[Epoch: 15| Loss: 1.7177| Accuracy: 76.60| Best Accuracy: 76.60]
[Epoch: 16| Loss: 1.7198| Accuracy: 76.26| Best Accuracy: 76.60]
[Epoch: 17| Loss: 1.7169| Accuracy: 76.59| Best Accuracy: 76.60]
[Epoch: 18| Loss: 1.7142| Accuracy: 76.74| Best Accuracy: 76.74]
[Epoch: 19| Loss: 1.7180| Accuracy: 76.50| Best Accuracy: 76.74]
[Epoch: 20| Loss: 1.7149| Accuracy: 76.62| Best Accuracy: 76.74]
[Epoch: 21| Loss: 1.7157| Accuracy: 76.45| Best Accuracy: 76.74]
[Epoch: 22| Loss: 1.7162| Accuracy: 76.50| Best Accuracy: 76.74]
[Epoch: 23| Loss: 1.7143| Accuracy: 76.57| Best Accuracy: 76.74]
[Epoch: 24| Loss: 1.7152| Accuracy: 76.45| Best Accuracy: 76.74]
[Epoch: 25| Loss: 1.7133| Accuracy: 76.55| Best Accuracy: 76.74]
[Epoch: 26| Loss: 1.7098| Accuracy: 77.21| Best Accuracy: 77.21]
[Epoch: 27| Loss: 1.7127| Accuracy: 76.81| Best Accuracy: 77.21]
[Epoch: 28| Loss: 1.7128| Accuracy: 76.83| Best Accuracy: 77.21]
[Epoch: 29| Loss: 1.7151| Accuracy: 76.71| Best Accuracy: 77.21]
[Epoch: 30| Loss: 1.7148| Accuracy: 76.73| Best Accuracy: 77.21]
[Epoch: 31| Loss: 1.7155| Accuracy: 76.49| Best Accuracy: 77.21]
[Epoch: 32| Loss: 1.7124| Accuracy: 76.89| Best Accuracy: 77.21]
[Epoch: 33| Loss: 1.7129| Accuracy: 76.80| Best Accuracy: 77.21]
[Epoch: 34| Loss: 1.7108| Accuracy: 76.86| Best Accuracy: 77.21]
[Epoch: 35| Loss: 1.7128| Accuracy: 76.73| Best Accuracy: 77.21]
[Epoch: 36| Loss: 1.7107| Accuracy: 76.88| Best Accuracy: 77.21]
[Epoch: 37| Loss: 1.7100| Accuracy: 77.07| Best Accuracy: 77.21]
[Epoch: 38| Loss: 1.7130| Accuracy: 77.04| Best Accuracy: 77.21]
[Epoch: 39| Loss: 1.7146| Accuracy: 76.74| Best Accuracy: 77.21]
[Epoch: 40| Loss: 1.7088| Accuracy: 77.21| Best Accuracy: 77.21]
[Epoch: 41| Loss: 1.7159| Accuracy: 76.49| Best Accuracy: 77.21]
[Epoch: 42| Loss: 1.7130| Accuracy: 76.77| Best Accuracy: 77.21]
[Epoch: 43| Loss: 1.7087| Accuracy: 77.21| Best Accuracy: 77.21]
[Epoch: 44| Loss: 1.7117| Accuracy: 76.93| Best Accuracy: 77.21]
[Epoch: 45| Loss: 1.7096| Accuracy: 76.97| Best Accuracy: 77.21]
[Epoch: 46| Loss: 1.7128| Accuracy: 76.90| Best Accuracy: 77.21]
[Epoch: 47| Loss: 1.7122| Accuracy: 76.81| Best Accuracy: 77.21]
[Epoch: 48| Loss: 1.7109| Accuracy: 76.86| Best Accuracy: 77.21]
[Epoch: 49| Loss: 1.7126| Accuracy: 77.10| Best Accuracy: 77.21]
adjusting learning rate to 0.001 ...
[Epoch: 50| Loss: 1.7314| Accuracy: 75.24| Best Accuracy: 77.21]
[Epoch: 51| Loss: 1.7362| Accuracy: 74.47| Best Accuracy: 77.21]
[Epoch: 52| Loss: 1.7269| Accuracy: 75.38| Best Accuracy: 77.21]
[Epoch: 53| Loss: 1.7204| Accuracy: 76.16| Best Accuracy: 77.21]
[Epoch: 54| Loss: 1.7342| Accuracy: 74.75| Best Accuracy: 77.21]
[Epoch: 55| Loss: 1.7334| Accuracy: 74.90| Best Accuracy: 77.21]
[Epoch: 56| Loss: 1.7190| Accuracy: 76.00| Best Accuracy: 77.21]
[Epoch: 57| Loss: 1.7284| Accuracy: 75.22| Best Accuracy: 77.21]
[Epoch: 58| Loss: 1.7246| Accuracy: 75.64| Best Accuracy: 77.21]
[Epoch: 59| Loss: 1.7356| Accuracy: 74.57| Best Accuracy: 77.21]
[Epoch: 60| Loss: 1.7300| Accuracy: 75.18| Best Accuracy: 77.21]
[Epoch: 61| Loss: 1.7326| Accuracy: 74.89| Best Accuracy: 77.21]
[Epoch: 62| Loss: 1.7436| Accuracy: 73.67| Best Accuracy: 77.21]
[Epoch: 63| Loss: 1.7221| Accuracy: 75.87| Best Accuracy: 77.21]
[Epoch: 64| Loss: 1.7246| Accuracy: 75.35| Best Accuracy: 77.21]
[Epoch: 65| Loss: 1.7255| Accuracy: 75.51| Best Accuracy: 77.21]
[Epoch: 66| Loss: 1.7235| Accuracy: 75.74| Best Accuracy: 77.21]
[Epoch: 67| Loss: 1.7214| Accuracy: 75.77| Best Accuracy: 77.21]
[Epoch: 68| Loss: 1.7255| Accuracy: 75.39| Best Accuracy: 77.21]
[Epoch: 69| Loss: 1.7343| Accuracy: 74.61| Best Accuracy: 77.21]
[Epoch: 70| Loss: 1.7205| Accuracy: 75.88| Best Accuracy: 77.21]
[Epoch: 71| Loss: 1.7333| Accuracy: 74.74| Best Accuracy: 77.21]
[Epoch: 72| Loss: 1.7194| Accuracy: 76.01| Best Accuracy: 77.21]
[Epoch: 73| Loss: 1.7312| Accuracy: 74.79| Best Accuracy: 77.21]
[Epoch: 74| Loss: 1.7268| Accuracy: 75.30| Best Accuracy: 77.21]
[Epoch: 75| Loss: 1.7291| Accuracy: 75.21| Best Accuracy: 77.21]
[Epoch: 76| Loss: 1.7226| Accuracy: 75.47| Best Accuracy: 77.21]
[Epoch: 77| Loss: 1.7201| Accuracy: 76.25| Best Accuracy: 77.21]
[Epoch: 78| Loss: 1.7163| Accuracy: 76.65| Best Accuracy: 77.21]
[Epoch: 79| Loss: 1.7163| Accuracy: 76.32| Best Accuracy: 77.21]
[Epoch: 80| Loss: 1.7340| Accuracy: 74.65| Best Accuracy: 77.21]
[Epoch: 81| Loss: 1.7139| Accuracy: 76.48| Best Accuracy: 77.21]
[Epoch: 82| Loss: 1.7171| Accuracy: 76.32| Best Accuracy: 77.21]
[Epoch: 83| Loss: 1.7238| Accuracy: 75.66| Best Accuracy: 77.21]
[Epoch: 84| Loss: 1.7186| Accuracy: 76.27| Best Accuracy: 77.21]
[Epoch: 85| Loss: 1.7340| Accuracy: 74.58| Best Accuracy: 77.21]
[Epoch: 86| Loss: 1.7200| Accuracy: 75.83| Best Accuracy: 77.21]
[Epoch: 87| Loss: 1.7135| Accuracy: 76.83| Best Accuracy: 77.21]
[Epoch: 88| Loss: 1.7195| Accuracy: 75.95| Best Accuracy: 77.21]
[Epoch: 89| Loss: 1.7187| Accuracy: 76.14| Best Accuracy: 77.21]
[Epoch: 90| Loss: 1.7105| Accuracy: 76.90| Best Accuracy: 77.21]
[Epoch: 91| Loss: 1.7124| Accuracy: 76.60| Best Accuracy: 77.21]
[Epoch: 92| Loss: 1.7261| Accuracy: 75.30| Best Accuracy: 77.21]
[Epoch: 93| Loss: 1.7206| Accuracy: 75.79| Best Accuracy: 77.21]
[Epoch: 94| Loss: 1.7206| Accuracy: 76.04| Best Accuracy: 77.21]
[Epoch: 95| Loss: 1.7098| Accuracy: 77.20| Best Accuracy: 77.21]
[Epoch: 96| Loss: 1.7145| Accuracy: 76.58| Best Accuracy: 77.21]
[Epoch: 97| Loss: 1.7188| Accuracy: 76.38| Best Accuracy: 77.21]
[Epoch: 98| Loss: 1.7213| Accuracy: 75.46| Best Accuracy: 77.21]
[Epoch: 99| Loss: 1.7171| Accuracy: 76.08| Best Accuracy: 77.21]
adjusting learning rate to 0.0005 ...
[Epoch: 100| Loss: 1.7056| Accuracy: 77.25| Best Accuracy: 77.25]
[Epoch: 101| Loss: 1.7064| Accuracy: 77.26| Best Accuracy: 77.26]
[Epoch: 102| Loss: 1.7047| Accuracy: 77.45| Best Accuracy: 77.45]
[Epoch: 103| Loss: 1.7064| Accuracy: 77.40| Best Accuracy: 77.45]
[Epoch: 104| Loss: 1.7126| Accuracy: 76.70| Best Accuracy: 77.45]
[Epoch: 105| Loss: 1.7016| Accuracy: 77.95| Best Accuracy: 77.95]
[Epoch: 106| Loss: 1.7092| Accuracy: 77.21| Best Accuracy: 77.95]
[Epoch: 107| Loss: 1.7031| Accuracy: 77.50| Best Accuracy: 77.95]
[Epoch: 108| Loss: 1.7082| Accuracy: 77.26| Best Accuracy: 77.95]
[Epoch: 109| Loss: 1.7029| Accuracy: 77.55| Best Accuracy: 77.95]
[Epoch: 110| Loss: 1.7078| Accuracy: 77.48| Best Accuracy: 77.95]
[Epoch: 111| Loss: 1.7065| Accuracy: 77.27| Best Accuracy: 77.95]
[Epoch: 112| Loss: 1.7077| Accuracy: 77.10| Best Accuracy: 77.95]
[Epoch: 113| Loss: 1.7109| Accuracy: 76.75| Best Accuracy: 77.95]
[Epoch: 114| Loss: 1.7060| Accuracy: 77.37| Best Accuracy: 77.95]
[Epoch: 115| Loss: 1.7045| Accuracy: 77.44| Best Accuracy: 77.95]
[Epoch: 116| Loss: 1.7091| Accuracy: 77.29| Best Accuracy: 77.95]
[Epoch: 117| Loss: 1.7039| Accuracy: 77.58| Best Accuracy: 77.95]
[Epoch: 118| Loss: 1.7091| Accuracy: 77.28| Best Accuracy: 77.95]
[Epoch: 119| Loss: 1.7099| Accuracy: 76.97| Best Accuracy: 77.95]
[Epoch: 120| Loss: 1.7051| Accuracy: 77.39| Best Accuracy: 77.95]
[Epoch: 121| Loss: 1.7027| Accuracy: 77.69| Best Accuracy: 77.95]
[Epoch: 122| Loss: 1.6995| Accuracy: 77.97| Best Accuracy: 77.97]
[Epoch: 123| Loss: 1.7106| Accuracy: 77.02| Best Accuracy: 77.97]
[Epoch: 124| Loss: 1.7063| Accuracy: 77.68| Best Accuracy: 77.97]
[Epoch: 125| Loss: 1.7078| Accuracy: 77.44| Best Accuracy: 77.97]
[Epoch: 126| Loss: 1.7127| Accuracy: 76.96| Best Accuracy: 77.97]
[Epoch: 127| Loss: 1.7068| Accuracy: 77.51| Best Accuracy: 77.97]
[Epoch: 128| Loss: 1.6970| Accuracy: 78.30| Best Accuracy: 78.30]
[Epoch: 129| Loss: 1.7016| Accuracy: 77.72| Best Accuracy: 78.30]
[Epoch: 130| Loss: 1.7116| Accuracy: 76.97| Best Accuracy: 78.30]
[Epoch: 131| Loss: 1.7066| Accuracy: 77.42| Best Accuracy: 78.30]
[Epoch: 132| Loss: 1.7003| Accuracy: 77.74| Best Accuracy: 78.30]
[Epoch: 133| Loss: 1.7026| Accuracy: 77.80| Best Accuracy: 78.30]
[Epoch: 134| Loss: 1.7056| Accuracy: 77.53| Best Accuracy: 78.30]
[Epoch: 135| Loss: 1.7090| Accuracy: 77.01| Best Accuracy: 78.30]
[Epoch: 136| Loss: 1.7077| Accuracy: 77.71| Best Accuracy: 78.30]
[Epoch: 137| Loss: 1.7019| Accuracy: 77.94| Best Accuracy: 78.30]
[Epoch: 138| Loss: 1.7068| Accuracy: 77.22| Best Accuracy: 78.30]
[Epoch: 139| Loss: 1.7078| Accuracy: 77.34| Best Accuracy: 78.30]
[Epoch: 140| Loss: 1.7075| Accuracy: 77.36| Best Accuracy: 78.30]
[Epoch: 141| Loss: 1.7070| Accuracy: 77.35| Best Accuracy: 78.30]
[Epoch: 142| Loss: 1.7057| Accuracy: 77.41| Best Accuracy: 78.30]
[Epoch: 143| Loss: 1.7016| Accuracy: 77.81| Best Accuracy: 78.30]
[Epoch: 144| Loss: 1.7025| Accuracy: 77.57| Best Accuracy: 78.30]
[Epoch: 145| Loss: 1.7023| Accuracy: 77.92| Best Accuracy: 78.30]
[Epoch: 146| Loss: 1.7006| Accuracy: 77.77| Best Accuracy: 78.30]
[Epoch: 147| Loss: 1.6983| Accuracy: 78.04| Best Accuracy: 78.30]
[Epoch: 148| Loss: 1.7033| Accuracy: 77.57| Best Accuracy: 78.30]
[Epoch: 149| Loss: 1.7016| Accuracy: 77.84| Best Accuracy: 78.30]
