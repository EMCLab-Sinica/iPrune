
==> Dump Arguments:
pruning method: energy
visible gpus: 4
sensitivity analysis: OFF
overall pruning ratio: 0.2
stage: 2

0
0

==> Setting params:
batch_size : 16
test_batch_size : 16
epochs : 150
lr_epochs : 50
lr : 0.0005
momentum : 0.9
weight_decay : 0.0001
no_cuda : False
seed : 1
log_interval : 100
arch : KWS_CNN_S
pretrained : saved_models/energy/KWS_CNN_S/stage_1.pth.tar
evaluate : False
retrain : False
prune : energy
prune_shape : vector
prune_target : None
stage : 2
debug : 1
candidates_pruning_ratios : [0.0, 0.0, 0.0, 0.0, 0.0]
gpus : [0]
visible_gpus : 4
learning_rate_list : [0.0005, 0.0001, 2e-05]
sa : True
sen_ana : False
overall_pruning_ratio : 0.2
cuda : False
====================

Load cached data ...
Load cached data ...
Load cached data ...
KWS_CNN_S(
  (conv1): Conv2d(1, 28, kernel_size=(10, 4), stride=(1, 1))
  (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(28, 28, kernel_size=(10, 4), stride=(2, 1))
  (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ip1): Linear(in_features=1792, out_features=16, bias=True)
  (ip2): Linear(in_features=16, out_features=128, bias=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ip3): Linear(in_features=128, out_features=12, bias=True)
  (relu): ReLU(inplace=True)
)
==> Start pruning ...
[0.015297064296206734, 0.054594538695627956, 0.0007402488854176833, 9.3481273521494e-05, 5.959060644891683e-05]
===> Starting Simulated Annealing...
[Iter 0] cur_temp: 100.00 | stop_temp: 20.00 | Loss: -inf | Best perf: -inf[Iter 1] cur_temp: 90.00 | stop_temp: 20.00 | Loss: -5.4909 | Best perf: -5.4909[Iter 2] cur_temp: 81.00 | stop_temp: 20.00 | Loss: -27.1973 | Best perf: -5.4909[Iter 3] cur_temp: 72.90 | stop_temp: 20.00 | Loss: -32.9266 | Best perf: -5.4909[Iter 4] cur_temp: 65.61 | stop_temp: 20.00 | Loss: -25.0438 | Best perf: -5.4909[Iter 5] cur_temp: 59.05 | stop_temp: 20.00 | Loss: -33.0037 | Best perf: -5.4909[Iter 6] cur_temp: 53.14 | stop_temp: 20.00 | Loss: -36.8944 | Best perf: -5.4909[Iter 7] cur_temp: 47.83 | stop_temp: 20.00 | Loss: -33.6776 | Best perf: -5.4909[Iter 8] cur_temp: 43.05 | stop_temp: 20.00 | Loss: -40.2165 | Best perf: -5.4909[Iter 9] cur_temp: 38.74 | stop_temp: 20.00 | Loss: -34.9737 | Best perf: -5.4909[Iter 10] cur_temp: 34.87 | stop_temp: 20.00 | Loss: -40.0611 | Best perf: -5.4909[Iter 11] cur_temp: 31.38 | stop_temp: 20.00 | Loss: -33.0505 | Best perf: -5.4909[Iter 12] cur_temp: 28.24 | stop_temp: 20.00 | Loss: -34.2330 | Best perf: -5.4909[Iter 13] cur_temp: 25.42 | stop_temp: 20.00 | Loss: -36.1918 | Best perf: -5.4909[Iter 14] cur_temp: 22.88 | stop_temp: 20.00 | Loss: -29.8180 | Best perf: -5.4909[Iter 15] cur_temp: 20.59 | stop_temp: 20.00 | Loss: -37.5339 | Best perf: -5.4909
------------------------------------------------------------------
- Intermittent-aware weight pruning info:
- Layer 0:        396 /       1120 (35.4%) weights are pruned
- Layer 1:      21840 /      31360 (69.6%) weights are pruned
- Layer 2:       6912 /      28672 (24.1%) weights are pruned
- Layer 3:        128 /       2048 ( 6.2%) weights are pruned
- Layer 4:          0 /       1536 ( 0.0%) weights are pruned
- Total  :      29276 /      64736 (45.2%) weights are pruned
------------------------------------------------------------------

[Epoch: 1| Loss: 0.4713| Accuracy: 85.13| Best Accuracy: 85.13]
[Epoch: 2| Loss: 0.4477| Accuracy: 85.99| Best Accuracy: 85.99]
[Epoch: 3| Loss: 0.4635| Accuracy: 85.87| Best Accuracy: 85.99]
[Epoch: 4| Loss: 0.4761| Accuracy: 85.66| Best Accuracy: 85.99]
[Epoch: 5| Loss: 0.4529| Accuracy: 86.30| Best Accuracy: 86.30]
[Epoch: 6| Loss: 0.4657| Accuracy: 85.73| Best Accuracy: 86.30]
[Epoch: 7| Loss: 0.4739| Accuracy: 85.66| Best Accuracy: 86.30]
[Epoch: 8| Loss: 0.4697| Accuracy: 86.07| Best Accuracy: 86.30]
[Epoch: 9| Loss: 0.4631| Accuracy: 86.20| Best Accuracy: 86.30]
[Epoch: 10| Loss: 0.4696| Accuracy: 86.20| Best Accuracy: 86.30]
[Epoch: 11| Loss: 0.4803| Accuracy: 85.79| Best Accuracy: 86.30]
[Epoch: 12| Loss: 0.4787| Accuracy: 85.77| Best Accuracy: 86.30]
[Epoch: 13| Loss: 0.5041| Accuracy: 85.77| Best Accuracy: 86.30]
[Epoch: 14| Loss: 0.5273| Accuracy: 84.72| Best Accuracy: 86.30]
[Epoch: 15| Loss: 0.4904| Accuracy: 85.91| Best Accuracy: 86.30]
[Epoch: 16| Loss: 0.5102| Accuracy: 85.13| Best Accuracy: 86.30]
[Epoch: 17| Loss: 0.5013| Accuracy: 85.97| Best Accuracy: 86.30]
[Epoch: 18| Loss: 0.5019| Accuracy: 85.69| Best Accuracy: 86.30]
[Epoch: 19| Loss: 0.5293| Accuracy: 85.52| Best Accuracy: 86.30]
[Epoch: 20| Loss: 0.5203| Accuracy: 85.15| Best Accuracy: 86.30]
[Epoch: 21| Loss: 0.5155| Accuracy: 85.64| Best Accuracy: 86.30]
[Epoch: 22| Loss: 0.5298| Accuracy: 86.09| Best Accuracy: 86.30]
[Epoch: 23| Loss: 0.5221| Accuracy: 85.44| Best Accuracy: 86.30]
[Epoch: 24| Loss: 0.5175| Accuracy: 85.48| Best Accuracy: 86.30]
[Epoch: 25| Loss: 0.5122| Accuracy: 85.81| Best Accuracy: 86.30]
[Epoch: 26| Loss: 0.5408| Accuracy: 85.32| Best Accuracy: 86.30]
[Epoch: 27| Loss: 0.5320| Accuracy: 85.77| Best Accuracy: 86.30]
[Epoch: 28| Loss: 0.5466| Accuracy: 85.01| Best Accuracy: 86.30]
[Epoch: 29| Loss: 0.5167| Accuracy: 85.81| Best Accuracy: 86.30]
[Epoch: 30| Loss: 0.5417| Accuracy: 85.79| Best Accuracy: 86.30]
[Epoch: 31| Loss: 0.5462| Accuracy: 85.19| Best Accuracy: 86.30]
[Epoch: 32| Loss: 0.5254| Accuracy: 85.58| Best Accuracy: 86.30]
[Epoch: 33| Loss: 0.5291| Accuracy: 85.46| Best Accuracy: 86.30]
[Epoch: 34| Loss: 0.5517| Accuracy: 84.99| Best Accuracy: 86.30]
[Epoch: 35| Loss: 0.5371| Accuracy: 85.75| Best Accuracy: 86.30]
[Epoch: 36| Loss: 0.5604| Accuracy: 84.76| Best Accuracy: 86.30]
[Epoch: 37| Loss: 0.5647| Accuracy: 84.68| Best Accuracy: 86.30]
[Epoch: 38| Loss: 0.5601| Accuracy: 85.58| Best Accuracy: 86.30]
[Epoch: 39| Loss: 0.5518| Accuracy: 85.73| Best Accuracy: 86.30]
[Epoch: 40| Loss: 0.5725| Accuracy: 85.24| Best Accuracy: 86.30]
[Epoch: 41| Loss: 0.5525| Accuracy: 84.74| Best Accuracy: 86.30]
[Epoch: 42| Loss: 0.5832| Accuracy: 84.64| Best Accuracy: 86.30]
[Epoch: 43| Loss: 0.5442| Accuracy: 85.48| Best Accuracy: 86.30]
[Epoch: 44| Loss: 0.5678| Accuracy: 84.97| Best Accuracy: 86.30]
[Epoch: 45| Loss: 0.5598| Accuracy: 85.62| Best Accuracy: 86.30]
[Epoch: 46| Loss: 0.5530| Accuracy: 85.09| Best Accuracy: 86.30]
[Epoch: 47| Loss: 0.5721| Accuracy: 85.11| Best Accuracy: 86.30]
[Epoch: 48| Loss: 0.5552| Accuracy: 85.11| Best Accuracy: 86.30]
[Epoch: 49| Loss: 0.5712| Accuracy: 85.77| Best Accuracy: 86.30]
adjusting learning rate to 0.0005 ...
[Epoch: 50| Loss: 0.5925| Accuracy: 84.83| Best Accuracy: 86.30]
[Epoch: 51| Loss: 0.5634| Accuracy: 85.09| Best Accuracy: 86.30]
[Epoch: 52| Loss: 0.5895| Accuracy: 84.89| Best Accuracy: 86.30]
[Epoch: 53| Loss: 0.5833| Accuracy: 84.36| Best Accuracy: 86.30]
[Epoch: 54| Loss: 0.5590| Accuracy: 85.21| Best Accuracy: 86.30]
[Epoch: 55| Loss: 0.5648| Accuracy: 85.15| Best Accuracy: 86.30]
[Epoch: 56| Loss: 0.5500| Accuracy: 85.28| Best Accuracy: 86.30]
[Epoch: 57| Loss: 0.5960| Accuracy: 84.91| Best Accuracy: 86.30]
[Epoch: 58| Loss: 0.5664| Accuracy: 85.32| Best Accuracy: 86.30]
[Epoch: 59| Loss: 0.5602| Accuracy: 85.46| Best Accuracy: 86.30]
[Epoch: 60| Loss: 0.5929| Accuracy: 84.87| Best Accuracy: 86.30]
[Epoch: 61| Loss: 0.5714| Accuracy: 84.87| Best Accuracy: 86.30]
[Epoch: 62| Loss: 0.5740| Accuracy: 85.30| Best Accuracy: 86.30]
[Epoch: 63| Loss: 0.5659| Accuracy: 85.17| Best Accuracy: 86.30]
[Epoch: 64| Loss: 0.5726| Accuracy: 85.50| Best Accuracy: 86.30]
[Epoch: 65| Loss: 0.6106| Accuracy: 84.68| Best Accuracy: 86.30]
[Epoch: 66| Loss: 0.5902| Accuracy: 85.52| Best Accuracy: 86.30]
[Epoch: 67| Loss: 0.5495| Accuracy: 85.60| Best Accuracy: 86.30]
[Epoch: 68| Loss: 0.6129| Accuracy: 84.64| Best Accuracy: 86.30]
[Epoch: 69| Loss: 0.5799| Accuracy: 85.38| Best Accuracy: 86.30]
[Epoch: 70| Loss: 0.5869| Accuracy: 85.24| Best Accuracy: 86.30]
[Epoch: 71| Loss: 0.5891| Accuracy: 85.28| Best Accuracy: 86.30]
[Epoch: 72| Loss: 0.5942| Accuracy: 85.19| Best Accuracy: 86.30]
[Epoch: 73| Loss: 0.5682| Accuracy: 85.75| Best Accuracy: 86.30]
[Epoch: 74| Loss: 0.5986| Accuracy: 84.72| Best Accuracy: 86.30]
[Epoch: 75| Loss: 0.6012| Accuracy: 85.05| Best Accuracy: 86.30]
[Epoch: 76| Loss: 0.5953| Accuracy: 84.21| Best Accuracy: 86.30]
[Epoch: 77| Loss: 0.5710| Accuracy: 85.73| Best Accuracy: 86.30]
[Epoch: 78| Loss: 0.6062| Accuracy: 85.17| Best Accuracy: 86.30]
[Epoch: 79| Loss: 0.6061| Accuracy: 84.93| Best Accuracy: 86.30]
[Epoch: 80| Loss: 0.5848| Accuracy: 85.15| Best Accuracy: 86.30]
[Epoch: 81| Loss: 0.6058| Accuracy: 84.48| Best Accuracy: 86.30]
[Epoch: 82| Loss: 0.6181| Accuracy: 85.01| Best Accuracy: 86.30]
[Epoch: 83| Loss: 0.6017| Accuracy: 84.25| Best Accuracy: 86.30]
[Epoch: 84| Loss: 0.6084| Accuracy: 85.03| Best Accuracy: 86.30]
[Epoch: 85| Loss: 0.6100| Accuracy: 85.09| Best Accuracy: 86.30]
[Epoch: 86| Loss: 0.6008| Accuracy: 84.83| Best Accuracy: 86.30]
[Epoch: 87| Loss: 0.6306| Accuracy: 84.76| Best Accuracy: 86.30]
[Epoch: 88| Loss: 0.6138| Accuracy: 85.24| Best Accuracy: 86.30]
[Epoch: 89| Loss: 0.6165| Accuracy: 85.11| Best Accuracy: 86.30]
[Epoch: 90| Loss: 0.6118| Accuracy: 84.93| Best Accuracy: 86.30]
[Epoch: 91| Loss: 0.6090| Accuracy: 84.36| Best Accuracy: 86.30]
[Epoch: 92| Loss: 0.6103| Accuracy: 85.09| Best Accuracy: 86.30]
[Epoch: 93| Loss: 0.6374| Accuracy: 84.46| Best Accuracy: 86.30]
[Epoch: 94| Loss: 0.6047| Accuracy: 84.89| Best Accuracy: 86.30]
[Epoch: 95| Loss: 0.6120| Accuracy: 85.56| Best Accuracy: 86.30]
[Epoch: 96| Loss: 0.6449| Accuracy: 84.97| Best Accuracy: 86.30]
[Epoch: 97| Loss: 0.5913| Accuracy: 85.17| Best Accuracy: 86.30]
[Epoch: 98| Loss: 0.6091| Accuracy: 84.79| Best Accuracy: 86.30]
[Epoch: 99| Loss: 0.6034| Accuracy: 84.91| Best Accuracy: 86.30]
adjusting learning rate to 0.0001 ...
[Epoch: 100| Loss: 0.5996| Accuracy: 85.85| Best Accuracy: 86.30]
[Epoch: 101| Loss: 0.6164| Accuracy: 85.77| Best Accuracy: 86.30]
[Epoch: 102| Loss: 0.5967| Accuracy: 85.36| Best Accuracy: 86.30]
[Epoch: 103| Loss: 0.6183| Accuracy: 85.26| Best Accuracy: 86.30]
[Epoch: 104| Loss: 0.6095| Accuracy: 85.60| Best Accuracy: 86.30]
[Epoch: 105| Loss: 0.6123| Accuracy: 85.54| Best Accuracy: 86.30]
[Epoch: 106| Loss: 0.6597| Accuracy: 85.62| Best Accuracy: 86.30]
[Epoch: 107| Loss: 0.6394| Accuracy: 85.44| Best Accuracy: 86.30]
[Epoch: 108| Loss: 0.6520| Accuracy: 85.60| Best Accuracy: 86.30]
[Epoch: 109| Loss: 0.6477| Accuracy: 85.42| Best Accuracy: 86.30]
[Epoch: 110| Loss: 0.6574| Accuracy: 85.15| Best Accuracy: 86.30]
[Epoch: 111| Loss: 0.6531| Accuracy: 85.09| Best Accuracy: 86.30]
[Epoch: 112| Loss: 0.6473| Accuracy: 85.69| Best Accuracy: 86.30]
[Epoch: 113| Loss: 0.6555| Accuracy: 84.85| Best Accuracy: 86.30]
[Epoch: 114| Loss: 0.6649| Accuracy: 85.54| Best Accuracy: 86.30]
[Epoch: 115| Loss: 0.6424| Accuracy: 85.34| Best Accuracy: 86.30]
[Epoch: 116| Loss: 0.6569| Accuracy: 84.99| Best Accuracy: 86.30]
[Epoch: 117| Loss: 0.6678| Accuracy: 85.42| Best Accuracy: 86.30]
[Epoch: 118| Loss: 0.6751| Accuracy: 85.19| Best Accuracy: 86.30]
[Epoch: 119| Loss: 0.6773| Accuracy: 85.17| Best Accuracy: 86.30]
[Epoch: 120| Loss: 0.6658| Accuracy: 85.21| Best Accuracy: 86.30]
[Epoch: 121| Loss: 0.6652| Accuracy: 85.01| Best Accuracy: 86.30]
[Epoch: 122| Loss: 0.6816| Accuracy: 84.95| Best Accuracy: 86.30]
[Epoch: 123| Loss: 0.6783| Accuracy: 85.01| Best Accuracy: 86.30]
[Epoch: 124| Loss: 0.7172| Accuracy: 85.01| Best Accuracy: 86.30]
[Epoch: 125| Loss: 0.6895| Accuracy: 85.24| Best Accuracy: 86.30]
[Epoch: 126| Loss: 0.6946| Accuracy: 85.03| Best Accuracy: 86.30]
[Epoch: 127| Loss: 0.6779| Accuracy: 84.83| Best Accuracy: 86.30]
[Epoch: 128| Loss: 0.7037| Accuracy: 84.79| Best Accuracy: 86.30]
[Epoch: 129| Loss: 0.7068| Accuracy: 84.74| Best Accuracy: 86.30]
[Epoch: 130| Loss: 0.7063| Accuracy: 84.58| Best Accuracy: 86.30]
[Epoch: 131| Loss: 0.7098| Accuracy: 85.11| Best Accuracy: 86.30]
[Epoch: 132| Loss: 0.6764| Accuracy: 85.56| Best Accuracy: 86.30]
[Epoch: 133| Loss: 0.7119| Accuracy: 84.91| Best Accuracy: 86.30]
[Epoch: 134| Loss: 0.7031| Accuracy: 84.93| Best Accuracy: 86.30]
[Epoch: 135| Loss: 0.7184| Accuracy: 84.29| Best Accuracy: 86.30]
[Epoch: 136| Loss: 0.6786| Accuracy: 85.11| Best Accuracy: 86.30]
[Epoch: 137| Loss: 0.7026| Accuracy: 84.83| Best Accuracy: 86.30]
[Epoch: 138| Loss: 0.6960| Accuracy: 85.13| Best Accuracy: 86.30]
[Epoch: 139| Loss: 0.6933| Accuracy: 84.56| Best Accuracy: 86.30]
[Epoch: 140| Loss: 0.7171| Accuracy: 84.62| Best Accuracy: 86.30]
[Epoch: 141| Loss: 0.6931| Accuracy: 84.91| Best Accuracy: 86.30]
[Epoch: 142| Loss: 0.7209| Accuracy: 85.32| Best Accuracy: 86.30]
[Epoch: 143| Loss: 0.6919| Accuracy: 85.13| Best Accuracy: 86.30]
[Epoch: 144| Loss: 0.6962| Accuracy: 84.97| Best Accuracy: 86.30]
[Epoch: 145| Loss: 0.7112| Accuracy: 85.17| Best Accuracy: 86.30]
[Epoch: 146| Loss: 0.6994| Accuracy: 85.07| Best Accuracy: 86.30]
[Epoch: 147| Loss: 0.6982| Accuracy: 84.91| Best Accuracy: 86.30]
[Epoch: 148| Loss: 0.7015| Accuracy: 85.32| Best Accuracy: 86.30]
[Epoch: 149| Loss: 0.7233| Accuracy: 84.27| Best Accuracy: 86.30]
adjusting learning rate to 2e-05 ...
[Epoch: 150| Loss: 0.7180| Accuracy: 84.81| Best Accuracy: 86.30]
